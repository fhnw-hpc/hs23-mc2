{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoV0YWpWeX0Y",
        "outputId": "709250cb-0d42-4bbc-9708-b8aefef07c54"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov 10 06:47:38 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVVwNqwjeYL0",
        "outputId": "da02e775-bfe9-427c-d9f7-82674e8feae5"
      },
      "source": [
        "%%writefile reduce.cu\n",
        "/* This program will performe a reduce vectorA (size N)\n",
        "* with the + operation.\n",
        "+---------+ \n",
        "|111111111| \n",
        "+---------+\n",
        "     |\n",
        "     N\n",
        "\n",
        "vectorA   = all Ones\n",
        "N = Sum of vectorA\n",
        "*/\n",
        "#include <iostream>\n",
        "#include <sstream>\n",
        "#include <stdlib.h>\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "\n",
        "// CUDA macro wrapper for checking errors\n",
        "#define gpuErrCheck(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char* file, int line, bool abort = true)\n",
        "{\n",
        "    if (code != cudaSuccess)\n",
        "    {\n",
        "        std::cout << \"GPUassert: \" << cudaGetErrorString(code) << \" \" << file << \" \" << line << std::endl;\n",
        "        if (abort)\n",
        "        {\n",
        "            exit(code);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "// CPU reduce\n",
        "void reduce(int* vectorA, int* sum, int size)\n",
        "{\n",
        "    sum[0] = 0;\n",
        "    for (int i = 0; i < size; i++)\n",
        "        sum[0] += vectorA[i];\n",
        "}\n",
        "\n",
        "\n",
        "// EXERCISE\n",
        "// Read: https://developer.nvidia.com/blog/faster-parallel-reductions-kepler/\n",
        "// Implement the reduce kernel based on the information\n",
        "// of the Nvidia blog post.\n",
        "// Implement both options, using no shared mem at all but global atomics\n",
        "// and using shared mem for the seconds recution phase.\n",
        "__global__ void cudaEvenFasterReduceAddition() {\n",
        "    //ToDo\n",
        "}\n",
        "\n",
        "\n",
        "// Already optimized reduce kernel using shared memory.\n",
        "__global__ void cudaReduceAddition(int* vectorA, int* sum)\n",
        "{\n",
        "    int globalIdx = 2 * blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    extern __shared__ int shmArray[];\n",
        "\n",
        "    shmArray[threadIdx.x] = vectorA[globalIdx];\n",
        "    shmArray[threadIdx.x + blockDim.x] = vectorA[globalIdx + blockDim.x];\n",
        "\n",
        "    for (int stride = blockDim.x; stride; stride >>= 1) {\n",
        "        if (threadIdx.x < stride) {\n",
        "            shmArray[threadIdx.x] += shmArray[threadIdx.x + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (threadIdx.x == 0) {\n",
        "        sum[blockIdx.x] = shmArray[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "// Compare result vectors\n",
        "int compareResultVec(int* vectorCPU, int* vectorGPU, int size)\n",
        "{\n",
        "    int error = 0;\n",
        "    for (int i = 0; i < size; i++)\n",
        "    {\n",
        "        error += abs(vectorCPU[i] - vectorGPU[i]);\n",
        "    }\n",
        "    if (error == 0)\n",
        "    {\n",
        "        cout << \"No errors. All good!\" << endl;\n",
        "        return 0;\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        cout << \"Accumulated error: \" << error << endl;\n",
        "        return -1;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "    // Define the size of the vector: 1048576 elements\n",
        "    const int N = 1 << 20;\n",
        "    const int NBR_BLOCK = 512;\n",
        "\n",
        "    // Allocate and prepare input\n",
        "    int* hostVectorA = new int[N];\n",
        "    int hostSumCPU[1];\n",
        "    int hostSumGPU[1];\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        hostVectorA[i] = 1;\n",
        "    }\n",
        "\n",
        "    // Alloc N times size of int at address of deviceVector[A-C]\n",
        "    int* deviceVectorA;\n",
        "    int* deviceSum;\n",
        "    gpuErrCheck(cudaMalloc(&deviceVectorA, N * sizeof(int)));\n",
        "    gpuErrCheck(cudaMalloc(&deviceSum, NBR_BLOCK* sizeof(int)));\n",
        "\n",
        "    // Copy data from host to device\n",
        "    gpuErrCheck(cudaMemcpy(deviceVectorA, hostVectorA, N * sizeof(int), cudaMemcpyHostToDevice));\n",
        "\n",
        "    // Run the vector kernel on the CPU\n",
        "    reduce(hostVectorA, hostSumCPU, N);\n",
        "\n",
        "    // Run kernel on all elements on the GPU\n",
        "    cudaReduceAddition <<<NBR_BLOCK, 1024, 2 * 1024 * sizeof(int)>>> (deviceVectorA, deviceSum);\n",
        "    gpuErrCheck(cudaPeekAtLastError());\n",
        "    cudaReduceAddition <<<1, NBR_BLOCK / 2, NBR_BLOCK * sizeof(int) >> > (deviceSum, deviceSum);\n",
        "    gpuErrCheck(cudaPeekAtLastError());\n",
        "\n",
        "    // Copy the result stored in device_y back to host_y\n",
        "    gpuErrCheck(cudaMemcpy(hostSumGPU, deviceSum, sizeof(int), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // Check for errors\n",
        "    auto isValid = compareResultVec(hostSumCPU, hostSumGPU, 1);\n",
        "\n",
        "    // Free memory on device\n",
        "    gpuErrCheck(cudaFree(deviceVectorA));\n",
        "    gpuErrCheck(cudaFree(deviceSum));\n",
        "\n",
        "    // Free memory on host\n",
        "    delete[] hostVectorA;\n",
        "\n",
        "    return isValid;\n",
        "}\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reduce.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXRAf_-9lub9"
      },
      "source": [
        "**Attention:** If you get a K80, you should compile it like this:\n",
        "`!nvcc -arch=sm_37 -o reduce reduce.cu`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lLHNUzHef9q"
      },
      "source": [
        "!nvcc -o reduce reduce.cu"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57IzTD97eqDX",
        "outputId": "6aceadaf-3698-4c1a-e9d9-0d69d0c4535c"
      },
      "source": [
        "!./reduce"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No errors. All good!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p-YpNLPnMEV"
      },
      "source": [
        "**Profiling on old GPU (K80)**\n",
        "\n",
        "*   `!nvprof --print-gpu-trace ./reduce`\n",
        "*   `!nvprof --analysis-metrics -o reduce_out.nvprof ./reduce`\n",
        "*   --> Use the Visual Profiler\n",
        "\n",
        "**Profiling on newer GPUs**\n",
        "\n",
        "*   `!nsys profile -f true -o reduce_out -t cuda ./reduce`\n",
        "*   `!nsys stats --report gputrace reduce.qdrep`\n",
        "*   `!nsys stats reduce.qdrep`\n",
        "*   `!ncu -f -o reduce --set full ./reduce`\n",
        "*   --> Nvidia Nsight Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:"
      ],
      "metadata": {
        "id": "oPbbe5AaUU4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof --print-gpu-trace ./reduce"
      ],
      "metadata": {
        "id": "8FszJbI5nVjz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e836e7a-2ad7-41ab-99a2-906a6fec252f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==253== NVPROF is profiling process 253, command: ./reduce\n",
            "No errors. All good!\n",
            "==253== Profiling application: ./reduce\n",
            "==253== Profiling result:\n",
            "   Start  Duration            Grid Size      Block Size     Regs*    SSMem*    DSMem*      Size  Throughput  SrcMemType  DstMemType           Device   Context    Stream  Name\n",
            "381.08ms  804.09us                    -               -         -         -         -  4.0000MB  4.8580GB/s    Pageable      Device     Tesla T4 (0)         1         7  [CUDA memcpy HtoD]\n",
            "385.04ms  92.031us            (512 1 1)      (1024 1 1)        16        0B  8.0000KB         -           -           -           -     Tesla T4 (0)         1         7  cudaReduceAddition(int*, int*) [114]\n",
            "385.14ms  6.3360us              (1 1 1)       (256 1 1)        16        0B  2.0000KB         -           -           -           -     Tesla T4 (0)         1         7  cudaReduceAddition(int*, int*) [116]\n",
            "385.14ms  2.0160us                    -               -         -         -         -        4B  1.8922MB/s      Device    Pageable     Tesla T4 (0)         1         7  [CUDA memcpy DtoH]\n",
            "\n",
            "Regs: Number of registers used per CUDA thread. This number includes registers used internally by the CUDA driver and/or tools and can be more than what the compiler shows.\n",
            "SSMem: Static shared memory allocated per CUDA block.\n",
            "DSMem: Dynamic shared memory allocated per CUDA block.\n",
            "SrcMemType: The type of source memory accessed by memory operation/copy\n",
            "DstMemType: The type of destination memory accessed by memory operation/copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZrPgfUgn1DX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5459d5d2-fdad-4f19-b5c5-49f516d13d1a"
      },
      "source": [
        "!nvprof --analysis-metrics -o reduce_out.nvprof ./reduce"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Warning: Skipping profiling on device 0 since profiling is not supported on devices with compute capability 7.5 and higher.\n",
            "                  Use NVIDIA Nsight Compute for GPU profiling and NVIDIA Nsight Systems for GPU tracing and CPU sampling.\n",
            "                  Refer https://developer.nvidia.com/tools-overview for more details.\n",
            "\n",
            "======== Warning: The option --aggregate-mode on has no effect. The --aggregate-mode <on|off> option applies to --events and --metrics options that follow it.\n",
            "======== Warning: The option --aggregate-mode off has no effect. The --aggregate-mode <on|off> option applies to --events and --metrics options that follow it.\n",
            "==264== NVPROF is profiling process 264, command: ./reduce\n",
            "No errors. All good!\n",
            "==264== Generated result file: /content/reduce_out.nvprof\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the file *reduce_out.nvprof* with the visual profiler of Nvidia, which you have locally installed."
      ],
      "metadata": {
        "id": "PTzsnXS4UYtc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lrwzKmITUmJp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}